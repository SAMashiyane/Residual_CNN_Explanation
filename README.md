# Residual_CNN_Explanation

In this notebook, I try to say the basic topics in order to understand the subject : *Residual,Architecture_of_Resnet50, BottleNeck, Linear BottleNeck,Inverted Residual... , * So that we can finally understand the famous models such as MobileNet architecture
 

<img src="[https://your-image-url.type](https://github.com/SAMashiyane/Residual_CNN_Explanation/blob/main/image/34R.png)" width="100" height="100">

In the table, there is a summary of the output size at every layer and the dimension of the convolutional kernels at every point in the structure.

![alt_text](https://github.com/SAMashiyane/Residual_CNN_Explanation/blob/main/image/Resnets_tabel.jpg)

Two-dimensional view:

![alt_text](https://github.com/SAMashiyane/Residual_CNN_Explanation/blob/main/image/aspect_resnet34.jpg)

Conv1:
![alt_text](https://github.com/SAMashiyane/Residual_CNN_Explanation/blob/main/image/conv1_resnet34.jpg)



